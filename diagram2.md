# Docker Container Data Flow Diagram

## GitHub PR Analyzer - Container Architecture & Data Flow

This diagram illustrates how data flows between Docker containers in the GitHub PR Analyzer system.

```mermaid
graph TB
    %% External Systems
    User[ğŸ‘¤ User/Client]
    GitHub[ğŸ™ GitHub API]
    OpenAI[ğŸ¤– OpenAI API]
    Anthropic[ğŸ§  Anthropic API]

    %% Docker Network
    subgraph DockerNetwork["ğŸ³ pr_analyzer_network"]
        
        %% API Container
        subgraph APIContainer["ğŸ“¦ pr_analyzer_api<br/>Port: 8000"]
            FastAPI[FastAPI Server]
            MainApp[app.main]
            Schemas[Pydantic Schemas]
            Routes[API Routes]
        end

        %% Database Container
        subgraph DBContainer["ğŸ“¦ pr_analyzer_postgres<br/>Port: 5432"]
            PostgreSQL[(PostgreSQL DB)]
            AnalysisTasks[analysis_tasks table]
            AnalysisResults[analysis_results table]
            CacheEntries[cache_entries table]
        end

        %% Redis Container
        subgraph RedisContainer["ğŸ“¦ pr_analyzer_redis<br/>Port: 6379"]
            RedisDB[(Redis)]
            TaskQueue[Task Queue<br/>DB 1]
            ResultBackend[Result Backend<br/>DB 2]
            VectorCache[Vector Cache<br/>DB 0]
        end

        %% Celery Worker Container
        subgraph WorkerContainer["ğŸ“¦ pr_analyzer_celery_worker"]
            CeleryWorker[Celery Worker]
            TaskProcessor[Task Processor]
            AIAgents[AI Agents]
            Coordinator[Analysis Coordinator]
            GitHubService[GitHub Service]
            VectorCacheService[Vector Cache Service]
        end

        %% Flower Container
        subgraph FlowerContainer["ğŸ“¦ pr_analyzer_flower<br/>Port: 5555"]
            FlowerUI[Flower Web UI]
            TaskMonitor[Task Monitor]
        end
    end

    %% Data Flow Connections
    User -->|1. HTTP Request<br/>POST /analyze-pr| APIContainer
    APIContainer -->|2. Store Task<br/>SQL INSERT| DBContainer
    APIContainer -->|3. Queue Task<br/>Redis LPUSH| RedisContainer
    
    RedisContainer -->|4. Task Pickup<br/>Redis BRPOP| WorkerContainer
    WorkerContainer -->|5. Update Status<br/>SQL UPDATE| DBContainer
    WorkerContainer -->|6. Fetch PR Data<br/>HTTP GET| GitHub
    
    WorkerContainer -->|7. Check Cache<br/>Vector Search| RedisContainer
    WorkerContainer -->|8. AI Analysis<br/>HTTP POST| OpenAI
    WorkerContainer -->|9. AI Analysis<br/>HTTP POST| Anthropic
    
    WorkerContainer -->|10. Store Results<br/>SQL INSERT| DBContainer
    WorkerContainer -->|11. Cache Results<br/>Redis SET| RedisContainer
    WorkerContainer -->|12. Task Complete<br/>Redis SET| RedisContainer
    
    APIContainer -->|13. Get Status<br/>Redis GET| RedisContainer
    APIContainer -->|14. Get Results<br/>SQL SELECT| DBContainer
    APIContainer -->|15. HTTP Response<br/>JSON| User
    
    FlowerContainer -->|Monitor Tasks<br/>Redis MONITOR| RedisContainer
    User -->|View Monitoring<br/>HTTP GET| FlowerContainer

    %% Container Health Checks
    APIContainer -.->|Health Check| DBContainer
    APIContainer -.->|Health Check| RedisContainer
    WorkerContainer -.->|Health Check| DBContainer
    WorkerContainer -.->|Health Check| RedisContainer
    FlowerContainer -.->|Health Check| RedisContainer
```

## Container-Specific Data Flow Details

### 1. API Container (`pr_analyzer_api`)
**Image**: `potpie-assignemnt-api`  
**Port**: `8000:8000`  
**Health**: `/health` endpoint

**Data Flow**:
```
Incoming:
â”œâ”€â”€ HTTP Requests from Users (Port 8000)
â”œâ”€â”€ Environment Variables (.env file)
â””â”€â”€ Docker Volumes (shared code)

Outgoing:
â”œâ”€â”€ Database Queries â†’ PostgreSQL Container (Port 5432)
â”œâ”€â”€ Redis Commands â†’ Redis Container (Port 6379)
â”œâ”€â”€ Task Queuing â†’ Redis DB 1
â”œâ”€â”€ Result Retrieval â†’ Redis DB 2
â””â”€â”€ HTTP Responses â†’ Users
```

**Internal Data Processing**:
- Request validation via Pydantic schemas
- Task creation and queuing
- Status checking and result retrieval
- Health monitoring of dependent services

### 2. PostgreSQL Container (`pr_analyzer_postgres`)
**Image**: `postgres:latest`  
**Port**: `5432:5432`  
**Health**: PostgreSQL health check

**Data Storage**:
```
Tables:
â”œâ”€â”€ analysis_tasks
â”‚   â”œâ”€â”€ Task metadata and status
â”‚   â”œâ”€â”€ PR information
â”‚   â””â”€â”€ Progress tracking
â”œâ”€â”€ analysis_results
â”‚   â”œâ”€â”€ Individual code issues
â”‚   â”œâ”€â”€ Severity levels
â”‚   â””â”€â”€ AI suggestions
â””â”€â”€ cache_entries
    â”œâ”€â”€ Vector embeddings
    â”œâ”€â”€ Cached analysis results
    â””â”€â”€ Usage statistics
```

**Data Flow**:
- Receives SQL queries from API and Worker containers
- Stores persistent data (tasks, results, cache)
- Provides ACID transactions for data consistency

### 3. Redis Container (`pr_analyzer_redis`)
**Image**: `redis:latest`  
**Port**: `6379:6379`  
**Health**: Redis ping check

**Database Organization**:
```
DB 0: Vector Cache
â”œâ”€â”€ Semantic embeddings
â”œâ”€â”€ Code similarity vectors
â””â”€â”€ Cache hit/miss tracking

DB 1: Celery Task Queue (Broker)
â”œâ”€â”€ Pending tasks
â”œâ”€â”€ Task routing
â””â”€â”€ Priority queues

DB 2: Celery Result Backend
â”œâ”€â”€ Task results
â”œâ”€â”€ Task status
â””â”€â”€ Progress updates
```

**Data Flow**:
- Task queuing: API â†’ Redis â†’ Worker
- Result storage: Worker â†’ Redis â†’ API
- Caching: Worker â†” Redis (bidirectional)
- Monitoring: Flower â† Redis

### 4. Celery Worker Container (`pr_analyzer_celery_worker`)
**Image**: `potpie-assignemnt-celery-worker`  
**Health**: Custom health check task

**Internal Components**:
```
Processing Pipeline:
â”œâ”€â”€ Task Reception (from Redis)
â”œâ”€â”€ GitHub API Integration
â”œâ”€â”€ AI Agent Coordination
â”‚   â”œâ”€â”€ Style Agent
â”‚   â”œâ”€â”€ Bug Detection Agent
â”‚   â”œâ”€â”€ Security Agent
â”‚   â””â”€â”€ Performance Agent
â”œâ”€â”€ Vector Cache Management
â”œâ”€â”€ Result Aggregation
â””â”€â”€ Status Updates
```

**Data Flow**:
```
Input Sources:
â”œâ”€â”€ Task Queue (Redis DB 1)
â”œâ”€â”€ GitHub API (PR data)
â”œâ”€â”€ Vector Cache (Redis DB 0)
â””â”€â”€ Database (task metadata)

Output Destinations:
â”œâ”€â”€ Analysis Results (PostgreSQL)
â”œâ”€â”€ Task Status (Redis DB 2)
â”œâ”€â”€ Vector Cache (Redis DB 0)
â”œâ”€â”€ AI APIs (OpenAI, Anthropic)
â””â”€â”€ GitHub API (PR comments)
```

### 5. Flower Container (`pr_analyzer_flower`)
**Image**: `potpie-assignemnt-celery-flower`  
**Port**: `5555:5555`  
**Health**: Flower web interface

**Monitoring Data**:
```
Real-time Metrics:
â”œâ”€â”€ Active Workers
â”œâ”€â”€ Task Queue Length  
â”œâ”€â”€ Task Processing Rate
â”œâ”€â”€ Failed Task Count
â”œâ”€â”€ Worker Resource Usage
â””â”€â”€ Task History
```

**Data Flow**:
- Connects to Redis for task monitoring
- Provides web interface for task visualization
- Real-time updates via WebSocket connections

## Inter-Container Communication

### Network Configuration
```yaml
Network: pr_analyzer_network
Type: bridge
Containers: All 5 containers
DNS: Automatic container name resolution
```

### Service Discovery
```
Container Communication:
â”œâ”€â”€ api â†’ postgres (database queries)
â”œâ”€â”€ api â†’ redis (task queuing, status)
â”œâ”€â”€ celery-worker â†’ postgres (result storage)
â”œâ”€â”€ celery-worker â†’ redis (task processing)
â”œâ”€â”€ celery-flower â†’ redis (monitoring)
â””â”€â”€ All containers â†’ external APIs
```

### Data Persistence
```
Volumes:
â”œâ”€â”€ postgres_data (PostgreSQL data)
â”œâ”€â”€ redis_data (Redis snapshots)
â””â”€â”€ Shared application code
```

## Container Startup Sequence

```mermaid
sequenceDiagram
    participant DC as Docker Compose
    participant PG as PostgreSQL
    participant RD as Redis
    participant API as FastAPI
    participant CW as Celery Worker
    participant FL as Flower

    DC->>PG: 1. Start PostgreSQL
    DC->>RD: 2. Start Redis
    
    Note over PG,RD: Wait for health checks
    
    DC->>API: 3. Start FastAPI
    API->>PG: 4. Create tables
    API->>RD: 5. Test connections
    
    DC->>CW: 6. Start Celery Worker
    CW->>PG: 7. Connect to database
    CW->>RD: 8. Connect to broker
    
    DC->>FL: 9. Start Flower
    FL->>RD: 10. Connect for monitoring
    
    Note over API,FL: All services healthy
```

## Environment Variables Flow

```
.env file â†’ Docker Compose â†’ Container Environment
â”œâ”€â”€ Database credentials
â”œâ”€â”€ Redis connection strings
â”œâ”€â”€ API keys (OpenAI, Anthropic, GitHub)
â”œâ”€â”€ Application settings
â””â”€â”€ Debug flags
```

## Data Security & Isolation

### Container Isolation
- Each container runs in isolated namespace
- Network communication only through defined ports
- File system isolation except shared volumes

### Data Protection
- Database credentials via environment variables
- API keys secured in container environment
- Redis password protection (if configured)
- Internal network communication only

## Scaling Considerations

### Horizontal Scaling
```
Scalable Components:
â”œâ”€â”€ Celery Workers (multiple instances)
â”œâ”€â”€ API Servers (load balancer needed)
â””â”€â”€ Redis (cluster mode)

Fixed Components:
â”œâ”€â”€ PostgreSQL (single master)
â””â”€â”€ Flower (single monitoring instance)
```

This Docker-specific data flow diagram shows how the containerized architecture enables scalable, maintainable, and secure processing of GitHub PR analysis requests through well-defined container boundaries and communication patterns.